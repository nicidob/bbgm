{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import sklearn.linear_model as linear\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_list = sorted(['hgt', 'stre', 'spd', 'jmp', 'endu', 'ins', 'dnk', 'ft', 'fg', 'tp', 'diq', 'oiq', 'drb', 'pss', 'reb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "pr = {}\n",
    "\n",
    "for fi,file in enumerate(['../gs_beta1.json','../gs_random1.json','../gs_rpd1.json','../gs_rpd2.json']):\n",
    "    with open(file,'rb') as fp:\n",
    "        data = json.load(fp)\n",
    "        # get player ratings\n",
    "        for p in data['players']:\n",
    "            for r in p['ratings']:\n",
    "                r2 = np.array([r[rating] for rating in ratings_list])\n",
    "                key = str(fi) + '_' + str(p['pid']) + '_' + str(r['season'])\n",
    "                pr[key] = r2\n",
    "        # get differences\n",
    "        for g in data['games']:\n",
    "            t1 = sum([_['pts'] for _ in g['teams'][0]['players']])\n",
    "            t2 = sum([_['pts'] for _ in g['teams'][1]['players']])\n",
    "            m1 = sum([_['min'] for _ in g['teams'][0]['players']])\n",
    "            m2 = sum([_['min'] for _ in g['teams'][1]['players']])\n",
    "            Y.append(t1-t2)\n",
    "            vec1 = [pr[str(fi) + '_' + str(_['pid']) + '_' + str(g['season'])]*_['min'] for _ in g['teams'][0]['players']]\n",
    "            vec1 = np.array(vec1).sum(0)/m1\n",
    "            vec2 = [pr[str(fi) + '_' + str(_['pid']) + '_' + str(g['season'])]*_['min'] for _ in g['teams'][1]['players']]\n",
    "            vec2 = np.array(vec2).sum(0)/m2\n",
    "            X.append(vec1-vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#clf_ft = PolynomialFeatures(include_bias=True)\n",
    "#X2 = clf_ft.fit_transform(X)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear.RidgeCV(alphas=np.logspace(-2,6,8),cv=3)\n",
    "#reg = linear.ElasticNetCV(cv=3,l1_ratio=np.logspace(-8,0,9),alphas=np.logspace(-4,4,80))\n",
    "reg.fit(X,Y)\n",
    "#min_v = reg.coef_[reg.coef_ >0].min()\n",
    "#min_v = abs(reg.coef_).mean()/7\n",
    "#reg.coef_[reg.coef_ < min_v] = min_v\n",
    "print(reg.score(X,Y)) # 0.146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reg.predict(X),Y,s=5,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_,np.log10(reg.alpha_)#,reg.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = {}\n",
    "for r,c in zip(reg.coef_,ratings_list):\n",
    "    print(np.round(r,3),c)\n",
    "    tmp[c] = np.round(r,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = pd.DataFrame(pr).T\n",
    "avg.columns = ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Intercept: \\n', reg.intercept_)\n",
    "# print('Coefficients: \\n', reg.coef_)\n",
    "\n",
    "# Adjust old ovrs for the ratings we're skipping\n",
    "# Recompute Ovr because we want the unscaled version, so scaling can be applied on top in JS\n",
    "avg['OvrOld'] = (5 * avg['hgt'] + 1 * avg['stre'] + 4 * avg['spd'] + 2 * avg['jmp'] + 1 * avg['endu'] + 1 * avg['ins'] + 2 * avg['dnk'] + 1 * avg['ft'] + 1 * avg['fg'] + 3 * avg['tp'] + 7 * avg['oiq'] + 3 * avg['diq'] + 3 * avg['drb'] + 3 * avg['pss'] + 1 * avg['reb']) / 38\n",
    "\n",
    "# Scale to match old ovr\n",
    "mean_old = avg.OvrOld.mean()\n",
    "std_old = avg.OvrOld.std()\n",
    "\n",
    "ovr_new_unscaled = reg.predict(avg[ratings_list])-reg.intercept_\n",
    "mean_new = ovr_new_unscaled.mean()\n",
    "std_new = ovr_new_unscaled.std()\n",
    "\n",
    "factor_mult = std_old / std_new\n",
    "factor_add = mean_old -mean_new*factor_mult\n",
    "print('factor_mult: \\n', factor_mult)\n",
    "print('factor_add: \\n', factor_add)\n",
    "avg['OvrNew'] = (ovr_new_unscaled) * factor_mult + factor_add\n",
    "# print(dataset.Ovr)\n",
    "# print(dataset.OvrNew)\n",
    "\n",
    "def formatThree(num):\n",
    "    return str(np.format_float_positional(num, precision=3, unique=False, fractional=False, trim='k'))\n",
    "\n",
    "print(avg[['OvrOld', 'OvrNew']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "print('(')\n",
    "alt_comp = 0\n",
    "for i in range(len(ratings_list)):\n",
    "    if i == len(ratings_list) - 1:\n",
    "        end_part = ''\n",
    "    else:\n",
    "        end_part = ' +'\n",
    "    idx = i\n",
    "    print('    ' + formatThree(factor_mult * reg.coef_[idx]) + ' * ratings.' + ratings_list[i] + end_part)\n",
    "    alt_comp = alt_comp + (factor_mult * reg.coef_[idx]) * avg[ratings_list[idx]]\n",
    "print(') + ' + formatThree(factor_add));\n",
    "alt_comp += factor_add\n",
    "\n",
    "# Plot\n",
    "avg.plot.hexbin(x='OvrOld', y='OvrNew', gridsize=40)\n",
    "plt.xlim(15, 85)\n",
    "plt.ylim(15, 85)\n",
    "plt.xlabel('Old Ovr')  \n",
    "plt.ylabel('New Ovr')  \n",
    "\n",
    "plt.plot([0, 100], [0, 100])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ovr_new_unscaled-mean_new) * factor_mult + factor_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ovr_new_unscaled) * factor_mult + (factor_add-mean_new*factor_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(avg['OvrNew'],40,alpha=0.5,label='new')\n",
    "plt.hist(avg['OvrOld'],40,alpha=0.5,label='old')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg['diff'] = avg['OvrNew']-avg['OvrOld']\n",
    "avg2 = avg[avg.index.map(lambda x: x[:2]=='2_' and x[-4:] == '2017')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg2[avg2.OvrOld > 60].sort_values('diff',0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.OLS(Y,sm.add_constant(pd.DataFrame(X,columns=ratings_list))).fit().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
